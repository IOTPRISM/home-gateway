{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time \n",
    "import os\n",
    "import pathlib\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.autograd import Variable \n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from statistics import mode, mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "directory = r'C:\\Users\\savva\\OneDrive\\Desktop\\npy'\n",
    "patient_ID = ['2zbyXzYNKPwiPtjaA2L64o.npy','3hY7Mp7u9YPo1xMARSxLhc.npy','4h1dAuzg9rdrhyojwxUS26.npy']\n",
    "colums1 = ['time','end time']\n",
    "colums2 = ['time','end time','Reading 1']\n",
    "colums3 = ['time','end time','Reading 1','Reading 2']\n",
    "door_key = 0\n",
    "switch = 0\n",
    "pir_key = 0\n",
    "hidden_size = 2\n",
    "hidden_size\n",
    "def Data_Loader(patient):\n",
    "    sensors_lst = []\n",
    "    column_names = []\n",
    "    for subdirectory in os.scandir(directory):\n",
    "        lock = False\n",
    "        for patients in os.scandir(subdirectory):\n",
    "            if str(pathlib.Path(patients)).endswith(patient_ID[0]):\n",
    "                if lock == False:\n",
    "                    column_names.append(str(os.path.basename(subdirectory)))\n",
    "                    lock == True                                              \n",
    "                sensors = np.load(pathlib.Path(patients))\n",
    "                sensors_lst.append(sensors)\n",
    "    sensors_pd = pd.DataFrame([sensors_lst],columns=column_names)\n",
    "    dataframe_lst = []\n",
    "    df = pd.DataFrame()\n",
    "    sensor_num = 2\n",
    "    Sensor_ID = column_names[sensor_num]\n",
    "    print(Sensor_ID)\n",
    "    for i in range(len(sensors_pd[Sensor_ID].loc[0])):\n",
    "        dataframe_lst.append(sensors_pd[Sensor_ID].loc[0][i])\n",
    "    if len(sensors_pd[Sensor_ID].loc[0][0]) == 2: \n",
    "        df = pd.DataFrame(dataframe_lst,columns = colums1)\n",
    "        df = df.drop(\"end time\", axis=1)\n",
    "    elif len(sensors_pd[Sensor_ID].loc[0][0]) == 3:    \n",
    "        df = pd.DataFrame(dataframe_lst,columns = colums2)\n",
    "        df = df.drop(\"end time\", axis=1)\n",
    "    elif len(sensors_pd[Sensor_ID].loc[0][0]) == 4:    \n",
    "        df = pd.DataFrame(dataframe_lst,columns = colums3)\n",
    "        df = df.drop(\"end time\", axis=1)\n",
    "    df['labels'] = 0\n",
    "    time = df['time']\n",
    "    anomaly_lst = []\n",
    "    col_name = df.columns\n",
    "    col_len = len(df.columns)\n",
    "    delta_t = []\n",
    "    n = df.columns[0]\n",
    "    df.drop(n, axis = 1, inplace = True)\n",
    "    for i in range(0,len(df)-1):\n",
    "        delta = abs(df.iloc[i,0] - df.iloc[i+1,0])\n",
    "        delta_t.append(delta)\n",
    "        i = i+1\n",
    "    df[n] = pd.Series(delta_t)\n",
    "    df.dropna(axis=1,inplace=False)\n",
    "    return df[0:10020],time\n",
    "\n",
    "def create_anomaly(time,n,id):\n",
    "    global pir_time\n",
    "    global pir_key\n",
    "    int = 0.0 # there are 20 locations\n",
    "    if pir_key == 0:\n",
    "        pir_time = time\n",
    "        s = [str(integer) for integer in pir_time]\n",
    "        a_string = \"\".join(s)\n",
    "        pir_time = float(a_string)\n",
    "        pir_key = 1\n",
    "    if pir_key == 1:\n",
    "        pir_time = 60.0\n",
    "        return [int] + [0] + [pir_time] \n",
    "\n",
    "def spike_anomaly(time,n,id): # for ambient temp\n",
    "    global pir_time\n",
    "    global pir_key\n",
    "    spikeint = 200.0 \n",
    "    room_id = 15.0\n",
    "    if pir_key == 0:\n",
    "        pir_time = time\n",
    "        s = [str(integer) for integer in pir_time]\n",
    "        a_string = \"\".join(s)\n",
    "        pir_time = float(a_string)\n",
    "        pir_key = 1\n",
    "    if pir_key == 1:\n",
    "        pir_time = 60.0\n",
    "        return [room_id] + [spikeint] + [0] + [pir_time] \n",
    "\n",
    "def variance_anomaly(time,n,id): # for ambient temp\n",
    "    global pir_time\n",
    "    global pir_key\n",
    "    variancen = randint(0.0,200.0)\n",
    "    variancen = variancen/10\n",
    "    if pir_key == 0:\n",
    "        pir_time = time\n",
    "        s = [str(integer) for integer in pir_time]\n",
    "        a_string = \"\".join(s)\n",
    "        pir_time = float(a_string)\n",
    "        pir_key = 1\n",
    "    if pir_key == 1:\n",
    "        pir_time = 30.0\n",
    "        return [id] + [variancen] + [0] + [pir_time] \n",
    "\n",
    "\n",
    "def anomaly_insertion(n,type,dataframe,id):\n",
    "    df = dataframe\n",
    "    anomaly_lst = []\n",
    "    col_name = df.columns\n",
    "    col_len = len(df.columns)\n",
    "    start_t = min(dataframe['time'])\n",
    "    end_t = max(dataframe['time'])\n",
    "    for i in range(n):\n",
    "        random_timestamp = random.randint(start_t,end_t)\n",
    "        if type == 'create':\n",
    "            zp_anomaly_lst = create_anomaly([random_timestamp],col_len,id)\n",
    "        if type == 'spike':\n",
    "            zp_anomaly_lst = spike_anomaly([random_timestamp],col_len,id)\n",
    "        if type == 'variance':\n",
    "            zp_anomaly_lst = variance_anomaly([random_timestamp],col_len,id)           \n",
    "        anomaly_lst.append(zp_anomaly_lst)\n",
    "\n",
    "    anomaly_df = pd.DataFrame(anomaly_lst,columns=col_name)\n",
    "    concat_df = pd.concat([dataframe,anomaly_df],ignore_index=True)\n",
    "    concat_df = concat_df.reset_index(drop=True)\n",
    "    return  concat_df\n",
    "\n",
    "dataT,time1 = Data_Loader(patient_ID[0])\n",
    "df1,time2 = Data_Loader(patient_ID[0])\n",
    "time1 = time1[0:10020]\n",
    "time2 = time2[0:10040]\n",
    "# dataV = anomaly_insertion(300,'create',df1,id = None)\n",
    "#dataV = anomaly_insertion(1,'spike',df1,id = None)\n",
    "dataV1 = anomaly_insertion(10,'variance',df1,5)\n",
    "dataV = anomaly_insertion(10,'variance',dataV1,7)\n",
    "dataT = dataT.astype('float32')\n",
    "dataV = dataV.astype('float32')\n",
    "\n",
    "train_data = [dataT]\n",
    "train_data = pd.concat(train_data)\n",
    "valid_data = dataV\n",
    "\n",
    "X_trainD = train_data.loc[:, train_data.columns != 'labels']\n",
    "X_valD = valid_data.loc[:, valid_data.columns != 'labels']\n",
    "Y_trainD = train_data.loc[:,'labels']\n",
    "Y_valD = valid_data.loc[:,'labels']\n",
    "\n",
    "X_train = torch.tensor(X_trainD.values).float()\n",
    "X_val =torch.tensor(X_valD.values).float()\n",
    "Y_train = torch.tensor(Y_trainD.values).float()\n",
    "Y_val = torch.tensor(Y_valD.values).float()\n",
    "\n",
    "Y_tensor = Y_train.clone().detach()\n",
    "new_shape = (len(Y_tensor), 1)\n",
    "Y_train = Y_tensor.view(new_shape)\n",
    "\n",
    "Y_tensor = Y_val.clone().detach()\n",
    "new_shape = (len(Y_tensor), 1)\n",
    "Y_val = Y_tensor.view(new_shape)\n",
    "\n",
    "torch.flatten(X_train)\n",
    "torch.flatten(Y_train)\n",
    "torch.flatten(X_val)\n",
    "torch.flatten(Y_val)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)        \n",
    "        self.fc4 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 3e-5)\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 1\n",
    "n_epochs = 1\n",
    "permutation_train = torch.randperm(X_train.size()[0])\n",
    "permutation_val = torch.randperm(X_val.size()[0])\n",
    "anomalies = []\n",
    "loss_history = []\n",
    "history = []\n",
    "\n",
    "def Train_AD():\n",
    "    for epoch in range(n_epochs):\n",
    "        accuracy = 0\n",
    "        accuracy_history = []\n",
    "        timer_plot = []\n",
    "        for i in range(0,len(train_data), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            indices = permutation_train[i:i+batch_size]\n",
    "            batch_x, batch_y = X_train[indices], Y_train[indices]\n",
    "            outputs = model.forward(batch_x)\n",
    "            loss = criterion(outputs,batch_y)\n",
    "            loss_history.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"-> Training :   \"+str(i)+\"/\"+str(X_train.size()[0]), end = \"\\r\")  \n",
    "        threshold = max(loss_history) - mode(loss_history)\n",
    "        for a in range(0,X_val.size()[0], batch_size):\n",
    "            start_time = time.time() \n",
    "            indicesV = permutation_val[a:a+batch_size]\n",
    "            batch_x_val, batch_y_val = X_val[indicesV], Y_val[indicesV]\n",
    "            output = model.forward(batch_x_val)\n",
    "            pred = torch.max(output,1)[1]\n",
    "            loss = criterion(output,batch_y_val)\n",
    "            history.append(loss.item())\n",
    "            if loss.item() >= threshold:\n",
    "                anomalies.append(batch_x_val)\n",
    "            time_elapsed = time.time() - start_time\n",
    "            timer_plot.append(1000*time_elapsed/len(X_val))\n",
    "            print(f\"-> Testing :     \"+str(a)+\"/\"+str(X_val.size()[0]), end = \"\\r\")\n",
    "        print(f\"Finished                                                                                         \")\n",
    "    return anomalies\n",
    "anomalies = Train_AD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 5.0000, 14.6000, 40.0000]]), tensor([[ 5.0000, 14.8000, 40.0000]]), tensor([[ 7.,  1., 40.]]), tensor([[ 7.0000, 13.5000, 40.0000]]), tensor([[ 5.,  3., 40.]]), tensor([[ 7.0000, 19.4000, 40.0000]]), tensor([[ 1., 22.,  1.]]), tensor([[18.0000, 23.6000, 15.0000]]), tensor([[ 5.0000,  4.2000, 40.0000]]), tensor([[ 2.0000, 25.5000,  0.0000]]), tensor([[ 1.0000, 21.1000,  1.0000]]), tensor([[18.0000, 24.7000,  0.0000]]), tensor([[ 5.,  9., 40.]]), tensor([[ 7.0000,  8.8000, 40.0000]]), tensor([[ 5.0000,  3.5000, 40.0000]]), tensor([[ 7.0000,  5.2000, 40.0000]]), tensor([[ 7.0000, 14.8000, 40.0000]]), tensor([[ 1.0000, 22.3000,  1.0000]]), tensor([[ 5.0000, 19.8000, 40.0000]]), tensor([[18.0000, 24.8000,  0.0000]]), tensor([[ 7.0000, 18.5000, 40.0000]]), tensor([[ 7.0000,  7.4000, 40.0000]]), tensor([[ 1.0000, 22.1000,  1.0000]]), tensor([[18.0000, 24.6000,  0.0000]]), tensor([[ 1.0000, 21.3000,  1.0000]]), tensor([[ 7.0000,  5.2000, 40.0000]]), tensor([[ 3.0000, 26.3000,  1.0000]]), tensor([[ 5.0000,  7.2000, 40.0000]]), tensor([[ 1.0000, 21.7000,  1.0000]]), tensor([[ 5.0000, 16.6000, 40.0000]]), tensor([[ 7.0000, 12.3000, 40.0000]]), tensor([[18.0000, 24.7000, 17.0000]]), tensor([[ 5.0000,  9.5000, 40.0000]])]\n"
     ]
    }
   ],
   "source": [
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "dates1 = pd.to_datetime(time1, utc=True, unit='s')\n",
    "dates2 = pd.to_datetime(time2, utc=True, unit='s')\n",
    "lstDateTime1 = dates1.to_list()\n",
    "lstDateTime2 = dates2.to_list()\n",
    "plt.figure(figsize=(16, 3))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.title('Training and Validation loss for half a month, with 20 variance anomalies')\n",
    "plt.plot(lstDateTime1,loss_history,label = 'training loss')\n",
    "plt.plot(lstDateTime2,history,label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.ylim([0,0.6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter = 0\n",
    "for i in range(len(anomalies)):\n",
    "    x = anomalies[i].numpy()\n",
    "    if x[0][2]==40.0:\n",
    "        counter = counter+1\n",
    "\n",
    "print(counter)\n",
    "print(counter/20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c879ca8ca5a815bc56826a638592fabe51b21a9759afa2f7d6c7754aed8eb254"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
